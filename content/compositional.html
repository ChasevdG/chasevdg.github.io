<!-- center header-->
<div class="subtab-about-content"></div>
<div class="subtab-about-header" style="text-align: center;">
  <h1>Clifford Hierarchical Algebras for Structural Equivariance</h1>
</div>
<div class="subtab-about-content">
<div class="subtab-about-text">
  <h2>Overview</h2>
  <p>
    Most research in Geometric Deep Learning (GDL) has focused on enforcing global equivariance in the neural network.
    However, most tasks such as images are compositional, where scenes are composed of objects, which are composed of parts, which are composed of subparts.
    These structures are often themselves equivariant, which induces a global equivariance in the system.
    Thus, the global equivariance, which is often the focus of GDL, does not capture the full <strong>compositional equivariance</strong> of the system.
  </p>
  <p>
    In the previous work of Shewmake et al. (--), they propose <i>hierarchically equivariant sparse coding</i> to capture the symmetries of the part-whole hierarchy.
    In this work, they motivate hierarchical equivariance using product groups and with elementwise products.
    Similarly, in Wang et al.(--) the formalize instead motivated by wreath products.
    <strong> By contrast, in this work, I propose a new algebraic framework for formalizing part-whole hierarchies.</strong>
  </p>
  <p>
    In GDL, emphasis is often put on representing symmetries as groups and equivariances as <i>group homomorphisms</i>.
    However, part-whole hierarchies can be intuitively represented as <i>algebras</i>, typically <i>rings</i>, which leads to a hierarchical equivariances being naturally expressed as <i>algebraic homomorphisms</i>.
    In formulating the problem algebraicly, I naturally motivate an architecture for learning part whole hierarchies, through the use of a neural field parameterized as a <i>ring</i>.
    To parameterize the ring, I use <i>Clifford Algebras</i> which naturally captures the rigid body motion transformations in images.
    Thus, we call our architecture <strong>Clifford Hierarchical Algebras for Structural Equivariance</strong>
  </p>
</div>
</div>
<div class="subtab-about-content">
  <div class="subtab-about-text">
    <h2>Motivation</h2>
    <p>
      The motivation behind geometric deep learning is often simplified to the idea that <i>"a rotation to the input should result in an equal rotation to the output"</i>.
      Thus, our task (Right) domain (images) has an <i>equivariant</i> symmetry with its codomain under rotations.
      An equivariant symmetry under a group \(G\) for a function, \(f : X \rightarrow Y\), is defined as,
    </p>
      <p>$$f(g \cdot x) = \phi(g) \cdot f(x)$$</p>
    <p>
      where \(g \in G\) is the group action (rotation) on the input \(x \in X\) and \(\phi(g)\) is the corresponding group action on the output \(f(x) \in Y\).
      In the above phrasing, rotation is assumed to act identically on the input and output.
      While this serves as a good starting point, it is often not enough to capture the full symmetries of the system.
    </p>
  </div>
  <div class="subtab-about-image">
    <img src="images/symmetry.GIF" alt="Subtab Image">
  </div>
  </div>
  <div class="subtab-about-content">
    <div class="subtab-about-image">
      <img src="images/CompEquiv.gif" alt="Subtab Image">
    </div>
    <div class="subtab-about-text">
    <p>
      In the case of images, the task is often compositional, where the image is composed of objects, which are composed of parts, which are composed of subparts.
      These sub-structures are often equivariant.
      For example, our task may be to segment an image composed of a cat and a dog.
      The cat can be translated and rotated, and the segmentation of the cat will follow that transformation.
      However, the segmentation of the dog will remain unchanged.
      Likewise, the segmentation map will follow the transformations to the dog and the segmentation of the cat will remain unchanged.
      Let's begin formalizing this idea.
    </p>
    </div>
  </div>
  <div class="subtab-about-content">
    <div class="subtab-about-text">
    <p>
      To translate this intuition into equations, we are given an image, \(I\), at wish to create a segmentation map, \(S\), using a neural network \(f\).
    </p>
    <p>$$S = f(I)$$</p>
    <p>
      Moreover, we know that our image is composed of objects,
    </p>
    <p>$$I = \{O_1, O_2, ..., O_n\}$$</p>
    <p>
      and that our segmentation map is composed of segmentations of these objects,
    </p>
    <p>$$S = \{S_1, S_2, ..., S_n\}$$</p>
    <p>
      This structure is shown on the right.
    </p>
  </div>
  <div class="subtab-about-image">
    <img src="images/structure.jpeg" alt="Subtab Image">
  </div>
</div>
<div class="subtab-about-content">
  <div class="subtab-about-image">
    <img src="images/composition.gif" alt="Subtab Image">
  </div>
  <div class="subtab-about-text">
  <p>
    This part-whole structure lets us intuitively define compositional equivariance.
    If a part is transformed, the segmentation of the part should also transform.
    Thus, a neural network is compositional equivariant if,
  </p>
  <p>$$f(O_1, ..., {\color{cyan}\mathbf{O_i}}, ..., O_n) = (S_1, ..., {\color{cyan}\mathbf{S_i}}, ... S_n)$$
    $$ \implies f((O_1, ..., {\color{orange}\mathbf{gO_i}}, ..., O_n)) = (S_1, ..., {\color{orange}\mathbf{\phi(g)S_i}}, ... S_n)$$
  </p>
  <p>
    where \(g\) is the group action (rotation) on the object \(O_i\) and \(\phi(g)\) is the corresponding group action on the segmentation \(S_i\).
    This is shown in the left image.
  <p>
    If we were to rotate the full image, the segmentation of the cat and dog would both rotate with the image.
    Thus, <i>if the task is compositionally equivariant, the task is also globally equivariant</i>. 
  </p>
  <p>
    (Note in the left image the objects are <i>not</i> rotated around the same center hence it is not equivalent to a global rotation)
  </p>
</div>
</div>
<p></p>
<div class="subtab-about-content">
<div class="subtab-about-text">
  <p>
    So far, we have shown scenes which are composed of objects. 
    However, objects are themselves composed of parts, which are composed of subparts.
    We call this hierarchical structure the <strong>part-whole hierarchy</strong>.
    <strong>Hierarchical equivariance</strong> is the idea of compositional equivariance extended to this hierarchy.
    Analogous to global rotations resulting in a rotation of all objects, a rotation at the object-level is equivalent to a rotation of all its parts.
    A neural network is then hierarchically equivariant if the output preserves the part-whole structure of the input.
  </p>
  <p>
    We must first construct the part-whole hierarchy in order to apply these ideas.
    Thus, we propose an unsupervised method of constructing the part-whole hierarchy.
    We will focus on this construction for the rest of the explanation.
  </p>
</div>
</div>
<div class="subtab-about-content">
  
  <div class="subtab-about-text">
  <h2>Part-Whole Hierarchy</h2>
  <p>
    Our intuition for the part-whole hierarchy is that we can rotate objects and parts independently from one another.
    While we can rotate an image, we can also rotate a face or tree within the image, or even the parts which compose the mouth.
  </p>
  <p>
    We can continue this process recursively down until we reach indivisible parts.
    In this case, the indivisible parts are the basic shapes -- lines, rectangles, circles and triangles -- which compose the objects.
    I will call these basic building blocks <strong>primitives</strong>, though one could also call them <i>atoms</i>.
  </p>
  <p>
    To be explicit about terms, continuing on, I will refer to the global level as a <strong>scene</strong>, and all intermediate levels as <strong>objects</strong>.
    The objects which compose a particular object are its <strong>parts</strong>.
  </p>
</div>
<div class="subtab-about-image">
  <img src="images/PHHier.gif" alt="Subtab Image">
</div>
</div>
<div class="subtab-about-content">
<div class="subtab-about-image">
<img src="images/tree.gif" alt="Subtab Image">
</div>
<div class="subtab-about-text">
<p>
These intuitions allows us to construct a tree structure of the part-whole hierarchy.
The full tree acts as a representation of the full scene and each branch represents a part-whole relationship.
Each node in the tree has a corresponding attribute -- such as position, orientation, or scale -- which we represent as a <strong>group element</strong> which transforms the object.
</p>
<p>
For a primer on group theory, see the <strong>Geometric Deep Learning</strong> tab in <strong>Interests</strong>.
</p>
<p>
As an example, an object can be represented at a node.
The group element at that node determines the position and orientation of that object, and the object can be rotated and translated by changing that group element.
The object itself is determined by the relative positions and orientations of the sub-tree below it with the exception of the leaves of the tree which are the primitive objects which have no sub-trees.
</p>
<p>
This tree structure hinges on two operations.
One is that an object is the composition of its parts, meaning that we need a <i>binding operator</i> which combines the parts into the object.
The other operation is the <i>group action</i> which transforms the object.
These two operations with our primitives allows us to construct a <strong>compositional algebra</strong>.
</p>
</div>

</div>
<div class="subtab-about-content">
<div class="subtab-about-text">
<h2>Binding</h2>
<p>
Our intuition motivates the need for a <strong>binding operator</strong> to compose the parts into the object.
The binding operator simply takes two objects and combines them into a new object.
While this is done by a simple <i>superposition</i>, ie summation, in this work, the binding operator can be more complex.
The summation is a symmetric binding operator, ie the order of the objects does not matter.
However, in general, the binding operator can be asymmetric.
For example, while in PDEs and physics superposition is often enough, in images there is often occlusion which makes the order important while binding.
</p>
<p>
The term "binding" is borrowed from neuroscience where it refers to the process by which the brain integrates information from different sources to create a unified perception or experience.
Binding operations are one of the pillars of Vector Symbolic Architectures (VSAs), or Hyperdimensional Computing. 
These architectures are based on the idea that the brain represents information as high-dimensional vectors; manipulating and combining these vectors using binding operations.
</p>
</div>
<div class="subtab-about-image">
<img src="images/tree.gif" alt="Subtab Image">
</div>
</div>
<div class="subtab-about-content">
<div class="subtab-about-image">
<img src="images/tree.gif" alt="Subtab Image">
</div>
<div class="subtab-about-text">
<h2>Group Product</h2>
<p>
The second operation is the <strong>group action</strong> which transforms the object.
If a group action is applied to two objects which are bound together, the group action should be applied to both objects.
That is, the group action should <i>distribute</i> over the binding operator.
</p>
<p>
$$
  g \cdot (O_1 \oplus O_2) = (g \cdot O_1) \oplus (g \cdot O_2)
$$
</p>
<p>
This is shown in the right image. 
The group product can act in two ways, either group element to group element or group element to object.
<p>
$$
  g_1 \cdot (g_2 \cdot O_2) = (g_1 \circ g_2) \cdot O_2
$$
</p>
<p>
  where \(\circ\) is the group product and \(\cdot\) is the group action on an object.
  These two properties lets us propogate the group action down the tree until we reach the primitives.
</p>
<p>
  In abstract algebra, an algebraic structure with two operations -- i.e. group action and binding -- is known as a <strong>ring</strong>.
  These 
</p>
</div>
</div>
<div class="subtab-about-content">
<div class="subtab-about-text">
<h2>To Be Continued</h2>

</div>
<div class="subtab-about-image">
<img src="images/tree.gif" alt="Subtab Image">
</div>